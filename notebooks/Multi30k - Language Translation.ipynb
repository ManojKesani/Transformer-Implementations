{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "import glob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LR = 1e-5\n",
    "NUM_EPOCHES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_english = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_german = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_english(text):\n",
    "    return [token.text for token in nlp_english.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_german(text):\n",
    "    return [token.text for token in nlp_german.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'guys', ',', 'my', 'name', 'Jeff']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_english(\"Hi guys, my name Jeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'dont', 'know', 'any', 'German']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_german(\"I dont know any German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH = torchtext.data.Field(tokenize=tokenizer_english, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMAN = torchtext.data.Field(tokenize=tokenizer_german, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = torchtext.datasets.Multi30k.splits(exts=(\".de\", \".en\"), fields=(GERMAN, ENGLISH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH.build_vocab(train, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMAN.build_vocab(train, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH vocab_size:  5893\n",
      "GERMAN vocab_size:  7854\n"
     ]
    }
   ],
   "source": [
    "print(\"ENGLISH vocab_size: \", len(ENGLISH.vocab))\n",
    "print(\"GERMAN vocab_size: \", len(GERMAN.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader, test_dataloader = torchtext.data.BucketIterator.splits(\n",
    "    (train, validation, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([11, 16])\n",
      "torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    print(batch_idx)\n",
    "    print(data.src.size())\n",
    "    print(data.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german2english(model, german_sentence, device=\"cpu\", max_len=100):\n",
    "    model.eval()\n",
    "    tokens = [token.text.lower() for token in nlp_german(german_sentence)]\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    \n",
    "    indexes = [GERMAN.vocab.stoi[token] for token in tokens]\n",
    "    indexes_tensor = torch.LongTensor(indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    english_sentence = [ENGLISH.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg = torch.LongTensor(english_sentence).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            word = model(indexes_tensor, trg)\n",
    "            \n",
    "        top = word.argmax(-1)[-1, :].item()\n",
    "        english_sentence.append(top)\n",
    "\n",
    "        if top == ENGLISH.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    english_sentence = [ENGLISH.vocab.itos[word] for word in english_sentence]\n",
    "    \n",
    "    return english_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH.vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import Transformer, Transformer_with_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = len(GERMAN.vocab)\n",
    "target_vocab_size = len(ENGLISH.vocab)\n",
    "embed_size = 512\n",
    "num_head = 16\n",
    "num_ff = 1024\n",
    "encoder_layers = 3\n",
    "decoder_layers = 3\n",
    "hidden_size = 1024\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "  (encoder_embed): Embedding(7854, 512)\n",
       "  (decoder_embed): Embedding(5893, 512)\n",
       "  (encoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoders): ModuleList(\n",
       "    (0): Transformer_Encoder(\n",
       "      (Norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (multi_attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Transformer_Encoder(\n",
       "      (Norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (multi_attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Transformer_Encoder(\n",
       "      (Norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (multi_attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Transformer_Decoder(\n",
       "      (masked_multiheadattention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (multiheadattention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (Norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer_Decoder(\n",
       "      (masked_multiheadattention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (multiheadattention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (Norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer_Decoder(\n",
       "      (masked_multiheadattention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (multiheadattention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (Norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final): Linear(in_features=512, out_features=5893, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(source_vocab_size, target_vocab_size, embed_size, num_head, num_ff, encoder_layers, decoder_layers, hidden_size, dropout=dropout, device=device).to(device)\n",
    "#model = Transformer_with_nn(source_vocab_size, target_vocab_size, embed_size, num_head, num_ff, encoder_layers, decoder_layers, dropout=dropout, device=device).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimenstions of Input Source Vector:  torch.Size([100, 16])\n",
      "Dimenstions of Input Target Vector:  torch.Size([90, 16])\n",
      "Dimenstions of Predicted Vector:  torch.Size([90, 16, 5893])\n",
      "Dimenstions of Input Source Vector:  torch.Size([100, 16])\n",
      "Dimenstions of Input Target Vector:  torch.Size([100, 16])\n",
      "Dimenstions of Predicted Vector:  torch.Size([100, 16, 5893])\n",
      "Dimenstions of Input Source Vector:  torch.Size([100, 16])\n",
      "Dimenstions of Input Target Vector:  torch.Size([110, 16])\n",
      "Dimenstions of Predicted Vector:  torch.Size([110, 16, 5893])\n"
     ]
    }
   ],
   "source": [
    "def test(size):\n",
    "    sample_in_x = torch.rand(100, BATCH_SIZE).type(torch.LongTensor).to(device)\n",
    "    sample_in_y = torch.rand(size, BATCH_SIZE).type(torch.LongTensor).to(device)\n",
    "    sample_out = model(sample_in_x, sample_in_y)\n",
    "    print(\"Dimenstions of Input Source Vector: \", sample_in_x.size())\n",
    "    print(\"Dimenstions of Input Target Vector: \", sample_in_y.size())\n",
    "    print(\"Dimenstions of Predicted Vector: \", sample_out.size())\n",
    "    \n",
    "test(90)\n",
    "test(100)\n",
    "test(110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.NLLLoss(ignore_index = ENGLISH.vocab.stoi[\"<pad>\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'men', '.', '<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 1 Train mean loss: 0.32972899\n",
      "       1 Test  mean loss: 0.27843789\n",
      "-------------------------------------------------\n",
      "['two', 'men', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are']\n",
      "-------------------------------------------------\n",
      "Epoch: 2 Train mean loss: 0.26828911\n",
      "       2 Test  mean loss: 0.24953339\n",
      "-------------------------------------------------\n",
      "['three', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "-------------------------------------------------\n",
      "Epoch: 3 Train mean loss: 0.24723121\n",
      "       3 Test  mean loss: 0.23144269\n",
      "-------------------------------------------------\n",
      "['three', 'people', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are']\n",
      "-------------------------------------------------\n",
      "Epoch: 4 Train mean loss: 0.23319985\n",
      "       4 Test  mean loss: 0.21881639\n",
      "-------------------------------------------------\n",
      "['three', 'people', 'are', 'standing', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are']\n",
      "-------------------------------------------------\n",
      "Epoch: 5 Train mean loss: 0.22254399\n",
      "       5 Test  mean loss: 0.20927943\n",
      "-------------------------------------------------\n",
      "['three', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young']\n",
      "-------------------------------------------------\n",
      "Epoch: 6 Train mean loss: 0.21400781\n",
      "       6 Test  mean loss: 0.20094207\n",
      "-------------------------------------------------\n",
      "['three', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young']\n",
      "-------------------------------------------------\n",
      "Epoch: 7 Train mean loss: 0.20666740\n",
      "       7 Test  mean loss: 0.19481404\n",
      "-------------------------------------------------\n",
      "['three', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young']\n",
      "-------------------------------------------------\n",
      "Epoch: 8 Train mean loss: 0.20016069\n",
      "       8 Test  mean loss: 0.18901263\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 9 Train mean loss: 0.19435659\n",
      "       9 Test  mean loss: 0.18350843\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 10 Train mean loss: 0.18895672\n",
      "       10 Test  mean loss: 0.17878025\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 11 Train mean loss: 0.18412777\n",
      "       11 Test  mean loss: 0.17446971\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 12 Train mean loss: 0.17973263\n",
      "       12 Test  mean loss: 0.17052987\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 13 Train mean loss: 0.17560984\n",
      "       13 Test  mean loss: 0.16669321\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 14 Train mean loss: 0.17186990\n",
      "       14 Test  mean loss: 0.16339138\n",
      "-------------------------------------------------\n",
      "['something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something']\n",
      "-------------------------------------------------\n",
      "Epoch: 15 Train mean loss: 0.16839419\n",
      "       15 Test  mean loss: 0.16022978\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 16 Train mean loss: 0.16485075\n",
      "       16 Test  mean loss: 0.15711781\n",
      "-------------------------------------------------\n",
      "['something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something']\n",
      "-------------------------------------------------\n",
      "Epoch: 17 Train mean loss: 0.16165233\n",
      "       17 Test  mean loss: 0.15398304\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 18 Train mean loss: 0.15847167\n",
      "       18 Test  mean loss: 0.15165904\n",
      "-------------------------------------------------\n",
      "['something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something', 'something']\n",
      "-------------------------------------------------\n",
      "Epoch: 19 Train mean loss: 0.15572296\n",
      "       19 Test  mean loss: 0.14931682\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 20 Train mean loss: 0.15281165\n",
      "       20 Test  mean loss: 0.14655816\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 21 Train mean loss: 0.15032176\n",
      "       21 Test  mean loss: 0.14471706\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 22 Train mean loss: 0.14769792\n",
      "       22 Test  mean loss: 0.14264360\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 23 Train mean loss: 0.14523239\n",
      "       23 Test  mean loss: 0.14054913\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 24 Train mean loss: 0.14301370\n",
      "       24 Test  mean loss: 0.13858195\n",
      "-------------------------------------------------\n",
      "['<eos>']\n",
      "-------------------------------------------------\n",
      "Epoch: 25 Train mean loss: 0.14070014\n",
      "       25 Test  mean loss: 0.13681595\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHES+1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    epoch_test_loss = 0  \n",
    "    \n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        x = data.src.to(device)\n",
    "        y = data.trg.to(device)\n",
    "        \n",
    "        y_pred = model(x, y[:-1, :])\n",
    "        \n",
    "        y_pred = y_pred.reshape(-1, y_pred.size(-1))\n",
    "        y = y[1:, :].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, data in enumerate(validation_dataloader):\n",
    "            x = data.src.to(device)\n",
    "            y = data.trg.to(device)\n",
    "        \n",
    "            y_pred = model(x, y[:-1, :])\n",
    "            y_pred = y_pred.reshape(-1, y_pred.size(-1))\n",
    "            y = y[1:, :].reshape(-1)\n",
    "        \n",
    "            loss = criterion(y_pred, y)\n",
    "        \n",
    "            epoch_test_loss += loss.item()\n",
    "    \n",
    "    epoch_train_loss = epoch_train_loss / len(train_dataloader.dataset)\n",
    "    epoch_test_loss = epoch_test_loss / len(validation_dataloader.dataset)\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print(german2english(model, \"einem orangefarbenen Hut, der etwas anstarrt.\", device=device)) # A man in an orange hat starring at something.\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, epoch_train_loss))\n",
    "        print(\"       {} Test  mean loss: {:.8f}\".format(epoch, epoch_test_loss))\n",
    "        print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '<eos>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german2english(model, \"Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\", device=device) # A man in an orange hat starring at something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"trained_models/language_translation_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
